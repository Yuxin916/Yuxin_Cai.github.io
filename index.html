<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuxin Cai Homepage</title>

    <meta name="author" content="Yuxin Cai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/robot-face-icon.png" type="image/x-icon"> 
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head> 
  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuxin Cai
                </p>
                <p>
                  Hi there! I’m a Ph.D. student in the <a href="https://lvchen.wixsite.com/automan">Automated Driving and Human-Machine System Lab</a> at
                  Nanyang Technological University (NTU), where I’m advised by <a href="https://scholar.google.com/citations?user=UKVs2CEAAAAJ&hl=en">Prof. Chen Lv</a>.
                  I’m also an AGS scholar in the Robotics and Autonomous Systems department, co-supervised by
                  <a href="https://www.a-star.edu.sg/i2r/about-i2r/i2r-management/yau-wei-yun">Dr. Wei-Yun Yau</a> at the Institute for Infocomm Research (I²R), A*STAR.
                  Right now, I’m visiting the <a href="https://safeai-lab.github.io/">Safe AI Lab</a> at Carnegie Mellon University,
                  hosted by <a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Prof. Ding Zhao</a>.
                  Before starting my Ph.D., I completed my B.Eng. (Hons) in Mechanical Engineering at NTU,
                  where I specialized in Robotics and Mechatronics.
                </p>
                <p>
                  My research interests lie in robot learning, with an emphasis on generalization across diverse tasks and 
                  environments. I am particularly interested in how agents can acquire transferable and scalable policies that 
                  remain robust under distribution shifts, unseen task variations, and dynamic multi-agent settings. My recent work 
                  explores hierarchical reasoning and vision-language grounding to enable more adaptive, interpretable, and 
                  generalizable decision-making in complex real-world contexts.
                  
                </p>
                <p style="text-align:center">
                  <a href="mailto:caiy0039@e.ntu.edu.sg">Email</a> &nbsp;/&nbsp;
                  <a href="data/cyx-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=_fEQHXQAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/YuxinCai_">Twitter</a> &nbsp;/&nbsp; 
                  <a href="https://github.com/Yuxin916/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/cyx.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" 
                  alt="profile photo" src="images/cyx.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
            
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <p>
                  <strong>2025.04</strong> I will be joining the <a href="https://safeai-lab.github.io/">Safe AI Lab</a> at CMU as a visiting student.
                </p>
              </td>
            </tr>
          </tbody>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
            <tr>
              <td style="padding:16px;width:50%;vertical-align:middle">
                <img src='research/4_CL_CoTNav_arc.png' width="100%" alt="CL-CoTNav">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2504.09000">
                  <span class="papertitle">CL-CoTNav: Closed-Loop Hierarchical Chain-of-Thought for Zero-Shot Object-Goal Navigation with Vision-Language Models</span>
                </a>
                <br>
                <strong>Yuxin Cai</strong>, Xiangkun He, Maonan Wang, Hongliang Guo, Wei-Yun Yau, Chen Lv  
                <br>
                <em>arXiv</em>, 2025  
                <p></p>
                <p>
                  A vision-language model (VLM)-driven framework that integrates structured chain-of-thought reasoning and
                  closed-loop feedback to enable zero-shot generalization in object navigation tasks.
                </p>
              </td>
            </tr>
          
            <tr>
              <td style="padding:16px;width:50%;vertical-align:middle">
                <img src='research/2_comat_arc.png' width="100%" alt="COMAT">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802580">
                  <span class="papertitle">Transformer-based Multi-Agent Reinforcement Learning for Generalization of Heterogeneous Multi-Robot Cooperation</span>
                </a>
                <br>
                <strong>Yuxin Cai</strong>, Xiangkun He, Hongliang Guo, Wei-Yun Yau, Chen Lv  
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024  
                &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font>
                <p></p>
                <p>
                  We propose a novel transformer-based multi-agent reinforcement learning framework that enables generalizable and cooperative behavior among heterogeneous robot teams across diverse task settings.
                </p>
              </td>
            </tr>
          
            <tr>
              <td style="padding:16px;width:50%;vertical-align:middle">
                <img src='research/3_IAHR_arc.png' width="100%" alt="IAHR">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10920267">
                  <span class="papertitle">Interaction-Aware Hierarchical Representation of Multi-Vehicle Reinforcement Learning for Cooperative Control in Dense Mixed Traffic</span>
                </a>
                <br>
                <strong>Yuxin Cai</strong>, Zhengxuan Liu, Xiangkun He, Zhiqiang Zuo, Wei-Yun Yau, Chen Lv  
                <br>
                <em>IEEE Intelligent Transportation Systems Conference (ITSC)</em>, 2024  
                &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font>
                <p></p>
                <p>
                  We introduce a hierarchical multi-agent reinforcement learning framework that models both inter-vehicle interactions and traffic-level dynamics to achieve robust and cooperative control for autonomous vehicles in dense, heterogeneous traffic scenarios.
                </p>
              </td>
            </tr>
            </tr>
            <tr>
              <td style="padding:16px;width:50%;vertical-align:middle">
                <img src='research/1_gaze.png' width="100%" alt="Gaze">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440167">
                  <span class="papertitle">Context-Aware Driver Attention Estimation Using Multi-Hierarchy Saliency Fusion With Gaze Tracking</span>
                </a>
                <br>
                Zhongxu Hu, <strong>Yuxin Cai</strong>, Qinghua Li, Kui Su, Chen Lv  
                <br>
                <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</em>, 2024  
                <p></p>
                <p>
                  We propose a context-aware driver attention estimation framework that fuses gaze tracking, saliency detection, and semantic scene understanding across multiple hierarchical levels to improve prediction accuracy in real-world driving scenarios.
                </p>
              </td>
            </tr>
            
          
          </tbody></table>
          
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Academic Services</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:8px 16px;vertical-align:top">
                <h3>Journal Reviewer</h3>
                <ul>
                  <li>IEEE Transactions on Intelligent Vehicles (T-IV), 2024</li>
                  <li>IEEE Transactions on Vehicular Technology (T-VT), 2023</li>
                  <li>IEEE Robotics and Automation Letters (RA-L), 2023-2024</li>
                </ul>
                <h3>Conference Reviewer</h3>
                <ul>
                  <li>IEEE International Conference on Robotics and Automation (ICRA), 2024</li>
                  <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023-2025</li>
                  <li>IEEE Intelligent Transportation Systems Conference (ITSC) 2024-2025</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          
          
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template from Jon Barron, <a href="https://github.com/jonbarron/website">jonbarron.com</a>.
                  </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
